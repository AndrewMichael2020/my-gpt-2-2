{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewMichael2020/my-gpt-2-2/blob/main/colab_train_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdRctc7gjdNF"
      },
      "source": [
        "# Healthcare \"SQL Agent\" Model â€” Colab Training Notebook\n",
        "\n",
        "This notebook trains a small decoder-only model **from scratch** to generate **one T-SQL SELECT** statement from a schema-aware question.\n",
        "\n",
        "Key fixes:\n",
        "- Deterministic ID vault for all identifiers\n",
        "- Hard `</SQL>` sentinel\n",
        "- Loss masking by construction (prompt masked, SQL supervised)\n",
        "- Token-based completion slicing\n"
      ],
      "id": "gdRctc7gjdNF"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install -U tokenizers==0.22.1\n"
      ],
      "metadata": {
        "id": "s98NkH7njfxv"
      },
      "id": "s98NkH7njfxv",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMZwOZhWjdNG"
      },
      "execution_count": 2,
      "outputs": [],
      "source": [
        "import os, re, json, time, random\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n"
      ],
      "id": "mMZwOZhWjdNG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gySCssaBjdNH",
        "outputId": "17bd8455-9016-4e76-993e-cd6610f0b5e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "\\\n",
        "# 1) Repro + Config\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "DATA_DIR = \"/content/data_sql_agent\"\n",
        "ART_DIR  = \"/content/artifacts_sql_agent\"\n",
        "CKPT_DIR = \"/content/checkpoints_sql_agent\"\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_JSONL = os.path.join(DATA_DIR, \"train.jsonl\")\n",
        "VAL_JSONL   = os.path.join(DATA_DIR, \"val.jsonl\")\n",
        "TOKENIZER_PATH = os.path.join(ART_DIR, \"tokenizer.json\")\n",
        "\n",
        "N_TRAIN = 5000\n",
        "N_VAL   = 200\n",
        "\n",
        "MAX_LEN = 384\n",
        "BATCH_SIZE = 8\n",
        "LR = 2e-4\n",
        "EPOCHS = 3\n",
        "WARMUP_STEPS = 200\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "D_MODEL = 384\n",
        "N_HEAD  = 6\n",
        "N_LAYER = 8\n",
        "D_FF    = 4 * D_MODEL\n",
        "DROPOUT = 0.1\n",
        "\n",
        "GEN_MAX_NEW_TOKENS = 160\n",
        "TEMPERATURE = 0.0\n",
        "\n",
        "SQL_END = \"</SQL>\"\n"
      ],
      "id": "gySCssaBjdNH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvwpaBMqjdNI"
      },
      "execution_count": 4,
      "outputs": [],
      "source": [
        "\\\n",
        "# 2) Schema + Templates\n",
        "SCHEMA_ID = \"healthcare_analytics\"\n",
        "SCHEMA_TEXT = (\n",
        "    \"Patients ( PatientID, FirstName, LastName, DateOfBirth, Gender, InsuranceProvider ); \"\n",
        "    \"Visits ( VisitID, PatientID, VisitDate, DepartmentID, ProviderID, VisitType, TotalCharge ); \"\n",
        "    \"Departments ( DepartmentID, DepartmentName, Location ); \"\n",
        "    \"Providers ( ProviderID, ProviderName, Specialty, DepartmentID ); \"\n",
        "    \"Diagnoses ( DiagnosisID, VisitID, ICDCode, DiagnosisDescription )\"\n",
        ")\n",
        "\n",
        "QUESTION_TEMPLATES = [\n",
        "    (\"How many visits did patient {PID} have in department {DID}?\",\n",
        "     \"SELECT COUNT(*) FROM Visits WHERE PatientID = {PID} AND DepartmentID = {DID};\"),\n",
        "    (\"What is the total charge for patient {PID}?\",\n",
        "     \"SELECT SUM(TotalCharge) FROM Visits WHERE PatientID = {PID};\"),\n",
        "    (\"Show monthly visit counts in {YEAR}\",\n",
        "     \"SELECT MONTH(VisitDate) AS [Month], COUNT(*) AS VisitCount \"\n",
        "     \"FROM Visits WHERE YEAR(VisitDate) = {YEAR} GROUP BY MONTH(VisitDate) ORDER BY [Month];\"),\n",
        "    (\"List providers in department {DID} ordered by name\",\n",
        "     \"SELECT ProviderID, ProviderName FROM Providers WHERE DepartmentID = {DID} ORDER BY ProviderName;\"),\n",
        "    (\"Count visits by department in {YEAR}\",\n",
        "     \"SELECT DepartmentID, COUNT(*) AS VisitCount FROM Visits \"\n",
        "     \"WHERE YEAR(VisitDate) = {YEAR} GROUP BY DepartmentID ORDER BY VisitCount DESC;\"),\n",
        "]\n",
        "\n",
        "def rand_id(min_v: int, max_v: int) -> int:\n",
        "    return random.randint(min_v, max_v)\n",
        "\n",
        "def rand_year() -> int:\n",
        "    return random.randint(2018, 2024)\n"
      ],
      "id": "CvwpaBMqjdNI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vCnPdKVjdNJ",
        "outputId": "60e2c6a9-3008-4d63-e2c0-0d47ea9d609e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: How many visits did patient 5432 have in department 25?\n",
            "Clean: How many visits did patient __ID_1__ have in department __ID_2__?\n",
            "ID Map: {'__ID_1__': '5432', '__ID_2__': '25'}\n"
          ]
        }
      ],
      "source": [
        "\\\n",
        "# 3) ID Vault (Aggressive, Deterministic)\n",
        "def extract_placeholders(question: str) -> Tuple[str, Dict[str, str]]:\n",
        "    id_map: Dict[str, str] = {}\n",
        "    id_counter = 1\n",
        "    date_counter = 1\n",
        "    year_counter = 1\n",
        "    q = question\n",
        "\n",
        "    date_pat = re.compile(r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\")\n",
        "\n",
        "    def repl_date(m):\n",
        "        nonlocal date_counter\n",
        "        value = m.group(0)\n",
        "        ph = f\"__DATE_{date_counter}__\"\n",
        "        id_map[ph] = value\n",
        "        date_counter += 1\n",
        "        return ph\n",
        "\n",
        "    q = date_pat.sub(repl_date, q)\n",
        "\n",
        "    int_pat = re.compile(r\"\\b\\d{1,6}\\b\")\n",
        "\n",
        "    def looks_like_year(num_str: str, text: str, start_idx: int) -> bool:\n",
        "        try:\n",
        "            n = int(num_str)\n",
        "        except:\n",
        "            return False\n",
        "        if not (1900 <= n <= 2100):\n",
        "            return False\n",
        "        left = max(0, start_idx - 18)\n",
        "        window = text[left:start_idx].lower()\n",
        "        triggers = [\" in \", \" year\", \" during\", \" for \", \" within\", \" by \"]\n",
        "        return any(t in window for t in triggers)\n",
        "\n",
        "    def repl_int(m):\n",
        "        nonlocal id_counter, year_counter\n",
        "        value = m.group(0)\n",
        "        start = m.start()\n",
        "        if looks_like_year(value, q, start):\n",
        "            ph = f\"__YEAR_{year_counter}__\"\n",
        "            id_map[ph] = value\n",
        "            year_counter += 1\n",
        "            return ph\n",
        "        ph = f\"__ID_{id_counter}__\"\n",
        "        id_map[ph] = value\n",
        "        id_counter += 1\n",
        "        return ph\n",
        "\n",
        "    q = int_pat.sub(repl_int, q)\n",
        "    return q, id_map\n",
        "\n",
        "def id_map_to_text(id_map: Dict[str, str]) -> str:\n",
        "    def idx(ph: str):\n",
        "        m = re.search(r\"_(\\d+)__\", ph)\n",
        "        return int(m.group(1)) if m else 0\n",
        "    items = sorted(id_map.items(), key=lambda kv: (kv[0].split(\"_\")[1], idx(kv[0])))\n",
        "    return \" ; \".join([f\"{k} = {v}\" for k, v in items]) if items else \"\"\n",
        "\n",
        "def finalize_sql(sql: str) -> str:\n",
        "    sql = sql.strip()\n",
        "    if not sql.endswith(\";\"):\n",
        "        sql = sql.rstrip(\";\") + \";\"\n",
        "    return sql + \" \" + SQL_END\n",
        "\n",
        "q0 = \"How many visits did patient 5432 have in department 25?\"\n",
        "clean0, map0 = extract_placeholders(q0)\n",
        "print(\"Original:\", q0)\n",
        "print(\"Clean:\", clean0)\n",
        "print(\"ID Map:\", map0)\n"
      ],
      "id": "1vCnPdKVjdNJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "essR2DayjdNK",
        "outputId": "ad10a3c6-9201-474f-e89e-799028694849"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating training dataset...\n",
            "Generating validation dataset...\n",
            "Generated 5000 training samples\n",
            "Generated 200 validation samples\n",
            "Example sample:\n",
            "{\n",
            "  \"schema_id\": \"healthcare_analytics\",\n",
            "  \"schema_text\": \"Patients ( PatientID, FirstName, LastName, DateOfBirth, Gender, InsuranceProvider ); Visits ( VisitID, PatientID, VisitDate, DepartmentID, ProviderID, VisitType, TotalCharge ); Departments ( DepartmentID, DepartmentName, Location ); Providers ( ProviderID, ProviderName, Specialty, DepartmentID ); Diagnoses ( DiagnosisID, VisitID, ICDCode, DiagnosisDescription )\",\n",
            "  \"question\": \"Count visits by department in __YEAR_1__\",\n",
            "  \"id_map\": \"__YEAR_1__ = 2020\",\n",
            "  \"sql\": \"SELECT DepartmentID, COUNT(*) AS VisitCount FROM Visits WHERE YEAR(VisitDate) = __YEAR_1__ GROUP BY DepartmentID ORDER BY VisitCount DESC; </SQL>\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\\\n",
        "# 4) Generate dataset (JSONL)\n",
        "def render_template(q_tmpl: str, sql_tmpl: str) -> str:\n",
        "    pid = str(rand_id(1000, 9999))\n",
        "    did = str(rand_id(1, 99))\n",
        "    yr  = str(rand_year())\n",
        "\n",
        "    q = q_tmpl.format(PID=pid, DID=did, YEAR=yr)\n",
        "    sql = sql_tmpl.format(PID=pid, DID=did, YEAR=yr)\n",
        "\n",
        "    clean_q, id_map = extract_placeholders(q)\n",
        "\n",
        "    val_to_ph = {}\n",
        "    for ph, val in id_map.items():\n",
        "        val_to_ph.setdefault(val, ph)\n",
        "\n",
        "    sql_ph = sql\n",
        "    for val, ph in val_to_ph.items():\n",
        "        sql_ph = re.sub(rf\"\\b{re.escape(val)}\\b\", ph, sql_ph)\n",
        "\n",
        "    sample = {\n",
        "        \"schema_id\": SCHEMA_ID,\n",
        "        \"schema_text\": SCHEMA_TEXT,\n",
        "        \"question\": clean_q,\n",
        "        \"id_map\": id_map_to_text(id_map),\n",
        "        \"sql\": finalize_sql(sql_ph),\n",
        "    }\n",
        "    return json.dumps(sample, ensure_ascii=False)\n",
        "\n",
        "def write_jsonl(path: str, n: int):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _ in range(n):\n",
        "            q_tmpl, sql_tmpl = random.choice(QUESTION_TEMPLATES)\n",
        "            f.write(render_template(q_tmpl, sql_tmpl) + \"\\n\")\n",
        "\n",
        "print(\"Generating training dataset...\")\n",
        "write_jsonl(TRAIN_JSONL, N_TRAIN)\n",
        "print(\"Generating validation dataset...\")\n",
        "write_jsonl(VAL_JSONL, N_VAL)\n",
        "\n",
        "print(f\"Generated {N_TRAIN} training samples\")\n",
        "print(f\"Generated {N_VAL} validation samples\")\n",
        "\n",
        "with open(TRAIN_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
        "    ex = json.loads(next(f))\n",
        "print(\"Example sample:\")\n",
        "print(json.dumps(ex, indent=2))\n"
      ],
      "id": "essR2DayjdNK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBSwAkt-jdNK",
        "outputId": "e59f9dc4-a715-464b-f730-8cb9594f9b15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokenizer with 100 special tokens...\n",
            "Tokenizer saved: /content/artifacts_sql_agent/tokenizer.json\n",
            "Vocab size: 981\n",
            "Tokens: ['SELECT', 'COUNT', '(*)', 'FROM', 'Visits', 'WHERE', 'PatientID', '=', '__ID_1__', ';']\n",
            "Tokens: ['SQL:', 'SELECT', '*', 'FROM', 'Visits', ';', '</SQL>']\n",
            "SQL: tokens: ['SQL:'] [7]\n"
          ]
        }
      ],
      "source": [
        "\\\n",
        "# 5) Tokenizer training (BPE) with special tokens\n",
        "def format_training_text(sample: Dict) -> str:\n",
        "    allowed = []\n",
        "    for ph in re.findall(r\"__\\\\w+_\\\\d+__\", sample['id_map']):\n",
        "        allowed.append(ph)\n",
        "    allowed_txt = ' '.join(allowed)\n",
        "    rules = 'RULES: Output must be exactly one T-SQL SELECT statement. Start with SELECT. End with a semicolon. Use only schema tables/columns. Use placeholders exactly as provided. Append </SQL> at the end.'\n",
        "    return (\n",
        "        f\"{rules}\\n\"\n",
        "        f\"SCHEMA: {sample['schema_text']}\\n\"\n",
        "        f\"QUESTION: {sample['question']}\\n\"\n",
        "        f\"ID_MAP: {sample['id_map']}\\n\"\n",
        "        f\"ALLOWED_PLACEHOLDERS: {allowed_txt}\\n\"\n",
        "        f\"SQL: {sample['sql']}\"\n",
        "    )\n",
        "\n",
        "def iter_training_texts(jsonl_path: str):\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            s = json.loads(line)\n",
        "            yield format_training_text(s)\n",
        "\n",
        "special_tokens = [\n",
        "    \"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\",\n",
        "    \"SCHEMA:\", \"QUESTION:\", \"ID_MAP:\", \"SQL:\",\n",
        "    SQL_END,\n",
        "    \"__ID_\", \"__DATE_\", \"__YEAR_\",\n",
        "]\n",
        "for i in range(1, 65):\n",
        "    special_tokens.append(f\"__ID_{i}__\")\n",
        "for i in range(1, 17):\n",
        "    special_tokens.append(f\"__DATE_{i}__\")\n",
        "for i in range(1, 9):\n",
        "    special_tokens.append(f\"__YEAR_{i}__\")\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = BpeTrainer(vocab_size=2000, min_frequency=2, special_tokens=special_tokens, show_progress=True)\n",
        "\n",
        "texts = list(iter_training_texts(TRAIN_JSONL))\n",
        "print(f\"Training tokenizer with {len(special_tokens)} special tokens...\")\n",
        "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
        "tokenizer.save(TOKENIZER_PATH)\n",
        "print(\"Tokenizer saved:\", TOKENIZER_PATH)\n",
        "print(\"Vocab size:\", tokenizer.get_vocab_size())\n",
        "\n",
        "t1 = \"SELECT COUNT(*) FROM Visits WHERE PatientID = __ID_1__;\"\n",
        "print(\"Tokens:\", tokenizer.encode(t1).tokens)\n",
        "\n",
        "t2 = \"SQL: SELECT * FROM Visits; </SQL>\"\n",
        "print(\"Tokens:\", tokenizer.encode(t2).tokens)\n",
        "print(\"SQL: tokens:\", tokenizer.encode(\"SQL:\").tokens, tokenizer.encode(\"SQL:\").ids)\n"
      ],
      "id": "eBSwAkt-jdNK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGb747DSjdNQ",
        "outputId": "b42251ff-efd0-4c6b-ae10-48e467a058bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First supervised label idx: 116\n"
          ]
        }
      ],
      "source": [
        "\\\n",
        "# 6) Dataset with loss masking BY CONSTRUCTION\n",
        "PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n",
        "SQL_END_ID = tokenizer.encode(SQL_END).ids[0]\n",
        "assert PAD_ID is not None and SQL_END_ID is not None\n",
        "\n",
        "def build_prompt_text(sample: Dict) -> str:\n",
        "    allowed = []\n",
        "    for ph in re.findall(r\"__\\\\w+_\\\\d+__\", sample['id_map']):\n",
        "        allowed.append(ph)\n",
        "    allowed_txt = ' '.join(allowed)\n",
        "    rules = 'RULES: Output must be exactly one T-SQL SELECT statement. Start with SELECT. End with a semicolon. Use only schema tables/columns. Use placeholders exactly as provided. Append </SQL> at the end.'\n",
        "    return (\n",
        "        f\"{rules}\\n\"\n",
        "        f\"SCHEMA: {sample['schema_text']}\\n\"\n",
        "        f\"QUESTION: {sample['question']}\\n\"\n",
        "        f\"ID_MAP: {sample['id_map']}\\n\"\n",
        "        f\"ALLOWED_PLACEHOLDERS: {allowed_txt}\\n\"\n",
        "        f\"SQL:\"\n",
        "    )\n",
        "\n",
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, jsonl_path: str, tokenizer: Tokenizer, max_len: int):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.samples = []\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                self.samples.append(json.loads(line))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        prompt = build_prompt_text(s)\n",
        "        sql = \" \" + s[\"sql\"].strip()\n",
        "\n",
        "        prompt_ids = self.tokenizer.encode(prompt).ids\n",
        "        sql_ids = self.tokenizer.encode(sql).ids\n",
        "\n",
        "        input_ids = (prompt_ids + sql_ids)[:self.max_len]\n",
        "        labels = ([-100] * len(prompt_ids) + sql_ids[:])[:self.max_len]\n",
        "\n",
        "        if len(input_ids) < self.max_len:\n",
        "            pad_len = self.max_len - len(input_ids)\n",
        "            input_ids += [PAD_ID] * pad_len\n",
        "            labels += [-100] * pad_len\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "def collate(batch):\n",
        "    x = torch.stack([b[\"input_ids\"] for b in batch], dim=0)\n",
        "    y = torch.stack([b[\"labels\"] for b in batch], dim=0)\n",
        "    return x, y\n",
        "\n",
        "train_ds = SQLDataset(TRAIN_JSONL, tokenizer, MAX_LEN)\n",
        "val_ds   = SQLDataset(VAL_JSONL, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "first = (y[0] != -100).nonzero(as_tuple=True)[0][0].item()\n",
        "print(\"First supervised label idx:\", first)\n",
        "# print(\"Decoded near boundary:\", tokenizer.decode(x[0].tolist(, skip_special_tokens=False)[max(0, first-10):first+25]))\n"
      ],
      "id": "wGb747DSjdNQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuX0rRNsjdNR",
        "outputId": "27b50cb0-a43b-4f06-f30c-ab5d6275f145"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 15,097,344 (15.10M)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\\\n",
        "# 7) Decoder-only model (TransformerEncoder + causal mask)\n",
        "class GPTSmall(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, n_head: int, n_layer: int, d_ff: int, dropout: float, max_len: int):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_head,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True,\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.blocks = nn.TransformerEncoder(enc_layer, num_layers=n_layer)\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        B, T = input_ids.shape\n",
        "        pos = torch.arange(0, T, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
        "        x = self.tok_emb(input_ids) + self.pos_emb(pos)\n",
        "        x = self.dropout(x)\n",
        "        causal = torch.triu(torch.ones(T, T, device=input_ids.device, dtype=torch.bool), diagonal=1)\n",
        "        x = self.blocks(x, mask=causal)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "model = GPTSmall(vocab_size, D_MODEL, N_HEAD, N_LAYER, D_FF, DROPOUT, MAX_LEN).to(DEVICE)\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {n_params:,} ({n_params/1e6:.2f}M)\")\n"
      ],
      "id": "YuX0rRNsjdNR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-lFowhDjdNS",
        "outputId": "09650659-b570-4c74-9c54-4897c8401653"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-271109821.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
            "/tmp/ipython-input-271109821.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1 step 100/625 loss=3.4958 lr=1.00e-04\n",
            "Epoch 1 step 200/625 loss=1.7778 lr=2.00e-04\n",
            "Epoch 1 step 300/625 loss=1.1859 lr=2.00e-04\n",
            "Epoch 1 step 400/625 loss=0.8897 lr=2.00e-04\n",
            "Epoch 1 step 500/625 loss=0.7119 lr=2.00e-04\n",
            "Epoch 1 step 600/625 loss=0.5932 lr=2.00e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-271109821.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Train Loss = 0.5695, Val Loss = 0.0000 (time 20.8s)\n",
            "Checkpoint saved: /content/checkpoints_sql_agent/checkpoint_epoch_1.pt\n",
            "Epoch 2 step 100/625 loss=0.0000 lr=2.00e-04\n",
            "Epoch 2 step 200/625 loss=0.0000 lr=2.00e-04\n",
            "Epoch 2 step 300/625 loss=0.0000 lr=2.00e-04\n",
            "Epoch 2 step 400/625 loss=0.0009 lr=2.00e-04\n",
            "Epoch 2 step 500/625 loss=0.0008 lr=2.00e-04\n",
            "Epoch 2 step 600/625 loss=0.0009 lr=2.00e-04\n",
            "\n",
            "Epoch 2: Train Loss = 0.0009, Val Loss = 0.0000 (time 20.1s)\n",
            "Checkpoint saved: /content/checkpoints_sql_agent/checkpoint_epoch_2.pt\n",
            "Epoch 3 step 100/625 loss=0.0000 lr=2.00e-04\n",
            "Epoch 3 step 200/625 loss=0.0000 lr=2.00e-04\n",
            "Epoch 3 step 300/625 loss=0.0000 lr=2.00e-04\n",
            "Epoch 3 step 400/625 loss=0.0000 lr=2.00e-04\n",
            "Epoch 3 step 500/625 loss=0.0000 lr=2.00e-04\n",
            "Epoch 3 step 600/625 loss=0.0000 lr=2.00e-04\n",
            "\n",
            "Epoch 3: Train Loss = 0.0000, Val Loss = 0.0000 (time 19.9s)\n",
            "Checkpoint saved: /content/checkpoints_sql_agent/checkpoint_epoch_3.pt\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "\\\n",
        "# 8) Training loop + checkpointing + warmup\n",
        "def get_lr(step: int, base_lr: float, warmup: int) -> float:\n",
        "    if step < warmup:\n",
        "        return base_lr * (step / max(1, warmup))\n",
        "    return base_lr\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
        "loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "def save_ckpt(epoch: int, global_step: int):\n",
        "    path = os.path.join(CKPT_DIR, f\"checkpoint_epoch_{epoch}.pt\")\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"global_step\": global_step,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }, path)\n",
        "    print(\"Checkpoint saved:\", path)\n",
        "\n",
        "def load_latest_ckpt() -> Tuple[int, int]:\n",
        "    files = [f for f in os.listdir(CKPT_DIR) if f.startswith(\"checkpoint_epoch_\") and f.endswith(\".pt\")]\n",
        "    if not files:\n",
        "        return 0, 0\n",
        "    files.sort(key=lambda x: int(re.search(r\"checkpoint_epoch_(\\d+)\\.pt\", x).group(1)))\n",
        "    latest = files[-1]\n",
        "    ckpt = torch.load(os.path.join(CKPT_DIR, latest), map_location=DEVICE)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    print(\"Loaded checkpoint:\", latest)\n",
        "    return ckpt[\"epoch\"], ckpt[\"global_step\"]\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loss(loader: DataLoader) -> float:\n",
        "    model.eval()\n",
        "    total = 0.0\n",
        "    n = 0\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
        "            logits = model(input_ids)\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_labels = labels[:, 1:].contiguous()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        total += loss.item()\n",
        "        n += 1\n",
        "    model.train()\n",
        "    return total / max(1, n)\n",
        "\n",
        "start_epoch, global_step = load_latest_ckpt()\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(start_epoch + 1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    running = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch_idx, (input_ids, labels) in enumerate(train_loader, start=1):\n",
        "        global_step += 1\n",
        "        lr_now = get_lr(global_step, LR, WARMUP_STEPS)\n",
        "        for pg in optimizer.param_groups:\n",
        "            pg[\"lr\"] = lr_now\n",
        "\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
        "            logits = model(input_ids)\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_labels = labels[:, 1:].contiguous()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running += loss.item()\n",
        "        n += 1\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch} step {batch_idx}/{len(train_loader)} loss={running/n:.4f} lr={lr_now:.2e}\")\n",
        "\n",
        "    train_loss = running / max(1, n)\n",
        "    val_loss = eval_loss(val_loader)\n",
        "    dt = time.time() - t0\n",
        "    print(f\"\\nEpoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f} (time {dt:.1f}s)\")\n",
        "    save_ckpt(epoch, global_step)\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "id": "_-lFowhDjdNS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G0cqVLujdNU",
        "outputId": "318fb560-b69b-4f18-c6dc-8e8cf1c4c727"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Q1] How many visits did patient 5432 have in department 25?\n",
            "Clean: How many visits did patient __ID_1__ have in department __ID_2__?\n",
            "ID Map: {'__ID_1__': '5432', '__ID_2__': '25'}\n",
            "SQL (placeholders): SELECT COUNT (*) FROM Visits WHERE PatientID = __ID_1__ AND DepartmentID = __ID_2__ ;\n",
            "Valid: True\n",
            "SQL (final): SELECT COUNT (*) FROM Visits WHERE PatientID = 5432 AND DepartmentID = 25 ;\n",
            "\n",
            "[Q2] What is the total charge for patient 7890?\n",
            "Clean: What is the total charge for patient __ID_1__?\n",
            "ID Map: {'__ID_1__': '7890'}\n",
            "SQL (placeholders): SELECT SUM ( TotalCharge ) FROM Visits WHERE PatientID = __ID_1__ ;\n",
            "Valid: True\n",
            "SQL (final): SELECT SUM ( TotalCharge ) FROM Visits WHERE PatientID = 7890 ;\n",
            "\n",
            "[Q3] Show monthly visit counts in 2023\n",
            "Clean: Show monthly visit counts in __YEAR_1__\n",
            "ID Map: {'__YEAR_1__': '2023'}\n",
            "SQL (placeholders): SELECT MONTH ( VisitDate ) AS [ Month ], COUNT (*) AS VisitCount FROM Visits WHERE YEAR ( VisitDate ) = __YEAR_1__ GROUP BY MONTH ( VisitDate ) ORDER BY [ Month ];\n",
            "Valid: True\n",
            "SQL (final): SELECT MONTH ( VisitDate ) AS [ Month ], COUNT (*) AS VisitCount FROM Visits WHERE YEAR ( VisitDate ) = 2023 GROUP BY MONTH ( VisitDate ) ORDER BY [ Month ];\n"
          ]
        }
      ],
      "source": [
        "\\\n",
        "# 9) Inference: token slicing + sentinel stop + extraction + validation\n",
        "def build_prompt(schema_text: str, clean_question: str, id_map_text: str) -> str:\n",
        "    rules = 'RULES: Output must be exactly one T-SQL SELECT statement. Start with SELECT. End with a semicolon. Use only schema tables/columns. Use placeholders exactly as provided. Append </SQL> at the end.'\n",
        "    allowed = ' '.join(re.findall(r\"__\\\\w+_\\\\d+__\", id_map_text))\n",
        "    return (\n",
        "        f\"{rules}\\n\"\n",
        "        f\"SCHEMA: {schema_text}\\n\"\n",
        "        f\"QUESTION: {clean_question}\\n\"\n",
        "        f\"ID_MAP: {id_map_text}\\n\"\n",
        "        f\"ALLOWED_PLACEHOLDERS: {allowed}\\n\"\n",
        "        f\"SQL:\"\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_ids(prompt_text: str, max_new_tokens: int = GEN_MAX_NEW_TOKENS) -> Tuple[List[int], int]:\n",
        "    model.eval()\n",
        "    prompt_ids = tokenizer.encode(prompt_text).ids\n",
        "    input_ids = torch.tensor([prompt_ids], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    allowed_ph = set(re.findall(r\"__\\w+_\\d+__\", prompt_text))\n",
        "    allowed_ph_ids = set()\n",
        "    for ph in allowed_ph:\n",
        "        enc = tokenizer.encode(ph)\n",
        "        if len(enc.ids) == 1:\n",
        "            allowed_ph_ids.add(enc.ids[0])\n",
        "\n",
        "    # Gather all known placeholder token ids (they are single-token special tokens in this setup)\n",
        "    all_ph_ids = []\n",
        "    vocab = tokenizer.get_vocab()\n",
        "    for tok, tid in vocab.items():\n",
        "        if re.fullmatch(r\"__\\w+_\\d+__\", tok):\n",
        "            all_ph_ids.append(tid)\n",
        "    all_ph_ids = torch.tensor(sorted(all_ph_ids), device=DEVICE, dtype=torch.long) if all_ph_ids else None\n",
        "\n",
        "    SEMI_ID = tokenizer.encode(\";\").ids[0]\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if input_ids.size(1) > MAX_LEN:\n",
        "            input_ids = input_ids[:, -MAX_LEN:]\n",
        "        logits = model(input_ids)\n",
        "        next_logits = logits[:, -1, :].clone()\n",
        "\n",
        "        # Constrain: forbid placeholders not in allowed set\n",
        "        if all_ph_ids is not None and all_ph_ids.numel() > 0:\n",
        "            forbid = []\n",
        "            for tid in all_ph_ids.tolist():\n",
        "                if tid not in allowed_ph_ids:\n",
        "                    forbid.append(tid)\n",
        "            if forbid:\n",
        "                next_logits[:, torch.tensor(forbid, device=DEVICE, dtype=torch.long)] = -1e9\n",
        "\n",
        "        if TEMPERATURE and TEMPERATURE > 0:\n",
        "            probs = torch.softmax(next_logits / TEMPERATURE, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            next_id = torch.argmax(next_logits, dim=-1, keepdim=True)\n",
        "\n",
        "        tok = int(next_id.item())\n",
        "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
        "\n",
        "        # Stop at semicolon, then append </SQL> for consistency\n",
        "        if tok == SEMI_ID:\n",
        "            input_ids = torch.cat([input_ids, torch.tensor([[SQL_END_ID]], device=DEVICE, dtype=torch.long)], dim=1)\n",
        "            break\n",
        "\n",
        "        if tok == SQL_END_ID:\n",
        "            break\n",
        "\n",
        "    return input_ids[0].tolist(), len(prompt_ids)\n",
        "\n",
        "def extract_sql(completion_text: str) -> str:\n",
        "    if SQL_END in completion_text:\n",
        "        completion_text = completion_text.split(SQL_END, 1)[0]\n",
        "    m = re.search(r\"\\bSELECT\\b\", completion_text, flags=re.IGNORECASE)\n",
        "    if not m:\n",
        "        return \"\"\n",
        "    tail = completion_text[m.start():]\n",
        "    semi = tail.find(\";\")\n",
        "    if semi == -1:\n",
        "        return \"\"\n",
        "    return tail[:semi+1].strip()\n",
        "\n",
        "DISALLOWED = re.compile(r\"\\b(INSERT|UPDATE|DELETE|MERGE|DROP|ALTER|CREATE|EXEC|GRANT|REVOKE|TRUNCATE)\\b\", re.IGNORECASE)\n",
        "KNOWN_TABLES = {\"Patients\",\"Visits\",\"Departments\",\"Providers\",\"Diagnoses\"}\n",
        "\n",
        "def validate_sql(sql: str, id_map: Dict[str,str]) -> Tuple[bool, List[str]]:\n",
        "    errs = []\n",
        "    s = sql.strip()\n",
        "    if not s:\n",
        "        errs.append(\"No SQL extracted.\")\n",
        "        return False, errs\n",
        "\n",
        "    if not re.match(r\"^SELECT\\b\", s, flags=re.IGNORECASE):\n",
        "        errs.append(\"Must start with SELECT.\")\n",
        "    if not s.endswith(\";\"):\n",
        "        errs.append(\"Must end with ';'.\")\n",
        "    if \";\" in s[:-1]:\n",
        "        errs.append(\"Must contain exactly one statement ending with ';'.\")\n",
        "    if DISALLOWED.search(s):\n",
        "        errs.append(\"Contains disallowed keyword.\")\n",
        "\n",
        "    if re.search(r\"=\\s*;\", s):\n",
        "        errs.append(\"Empty predicate value (found '= ;').\")\n",
        "    if re.search(r\"=\\s*(AND|OR|GROUP\\b|ORDER\\b|HAVING\\b|$)\", s, flags=re.IGNORECASE):\n",
        "        errs.append(\"Empty predicate value (found '= AND/OR/GROUP/ORDER').\")\n",
        "\n",
        "    used = re.findall(r\"__\\w+_\\d+__\", s)\n",
        "    used_set = set(used)\n",
        "\n",
        "    for ph in used_set:\n",
        "        if ph not in id_map:\n",
        "            errs.append(f\"Unknown placeholder: {ph}\")\n",
        "\n",
        "    required = set(id_map.keys())\n",
        "    missing = sorted(list(required - used_set))\n",
        "    if missing:\n",
        "        errs.append(f\"Missing required placeholder(s): {', '.join(missing)}\")\n",
        "\n",
        "    for ph in used_set:\n",
        "        if used.count(ph) > 3:\n",
        "            errs.append(f\"Placeholder repeated too many times: {ph}\")\n",
        "\n",
        "    if not any(re.search(rf\"\\b{t}\\b\", s) for t in KNOWN_TABLES):\n",
        "        errs.append(\"No known tables found in SQL.\")\n",
        "\n",
        "    return len(errs) == 0, errs\n",
        "\n",
        "def render_ids(sql: str, id_map: Dict[str,str]) -> str:\n",
        "    out = sql\n",
        "    for ph in sorted(id_map.keys(), key=len, reverse=True):\n",
        "        out = out.replace(ph, id_map[ph])\n",
        "    return out\n",
        "\n",
        "def generate_sql(question: str) -> Dict:\n",
        "    clean_q, id_map = extract_placeholders(question)\n",
        "    id_map_text = id_map_to_text(id_map)\n",
        "    prompt = build_prompt(SCHEMA_TEXT, clean_q, id_map_text)\n",
        "\n",
        "    all_ids, prompt_len = generate_ids(prompt)\n",
        "    completion_ids = all_ids[prompt_len:]  # token-based slicing\n",
        "    completion_text = tokenizer.decode(completion_ids, skip_special_tokens=False)\n",
        "\n",
        "    sql_ph = extract_sql(completion_text)\n",
        "    ok, errs = validate_sql(sql_ph, id_map)\n",
        "    sql_final = render_ids(sql_ph, id_map) if ok else \"\"\n",
        "\n",
        "    return {\n",
        "        \"original\": question,\n",
        "        \"clean\": clean_q,\n",
        "        \"id_map\": id_map,\n",
        "        \"completion\": completion_text,\n",
        "        \"sql_placeholders\": sql_ph,\n",
        "        \"valid\": ok,\n",
        "        \"errors\": errs,\n",
        "        \"sql_final\": sql_final,\n",
        "    }\n",
        "\n",
        "tests = [\n",
        "    \"How many visits did patient 5432 have in department 25?\",\n",
        "    \"What is the total charge for patient 7890?\",\n",
        "    \"Show monthly visit counts in 2023\",\n",
        "]\n",
        "\n",
        "for i, q in enumerate(tests, 1):\n",
        "    r = generate_sql(q)\n",
        "    print(f\"\\n[Q{i}] {q}\")\n",
        "    print(\"Clean:\", r[\"clean\"])\n",
        "    print(\"ID Map:\", r[\"id_map\"])\n",
        "    print(\"SQL (placeholders):\", r[\"sql_placeholders\"])\n",
        "    print(\"Valid:\", r[\"valid\"])\n",
        "    if not r[\"valid\"]:\n",
        "        print(\"Errors:\", r[\"errors\"])\n",
        "        print(\"Completion preview:\", r[\"completion\"][:220])\n",
        "    else:\n",
        "        print(\"SQL (final):\", r[\"sql_final\"])\n"
      ],
      "id": "7G0cqVLujdNU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiBH67FUjdNW",
        "outputId": "b394edfc-cd76-46de-ef09-bfa5489436dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pass rate: 100/100 = 100.0%\n"
          ]
        }
      ],
      "source": [
        "\\\n",
        "# 10) Pass-rate evaluation (synthetic)\n",
        "def sample_questions(n: int = 100) -> List[str]:\n",
        "    out = []\n",
        "    for _ in range(n):\n",
        "        q_tmpl, _ = random.choice(QUESTION_TEMPLATES)\n",
        "        pid = str(rand_id(1000, 9999))\n",
        "        did = str(rand_id(1, 99))\n",
        "        yr  = str(rand_year())\n",
        "        out.append(q_tmpl.format(PID=pid, DID=did, YEAR=yr))\n",
        "    return out\n",
        "\n",
        "def eval_pass_rate(n: int = 100):\n",
        "    qs = sample_questions(n)\n",
        "    passed = 0\n",
        "    reasons = {}\n",
        "    for q in qs:\n",
        "        r = generate_sql(q)\n",
        "        if r[\"valid\"]:\n",
        "            passed += 1\n",
        "        else:\n",
        "            key = r[\"errors\"][0] if r[\"errors\"] else \"Unknown\"\n",
        "            reasons[key] = reasons.get(key, 0) + 1\n",
        "    print(f\"Pass rate: {passed}/{n} = {passed/n:.1%}\")\n",
        "    for k, v in sorted(reasons.items(), key=lambda kv: kv[1], reverse=True)[:10]:\n",
        "        print(f\"- {k}: {v}\")\n",
        "\n",
        "eval_pass_rate(100)\n"
      ],
      "id": "RiBH67FUjdNW"
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Interactive Q&A cell ===\n",
        "# # Ask any question; it will print the cleaned question, placeholders, and final SQL.\n",
        "\n",
        "# def ask(q: str):\n",
        "#     r = generate_sql(q)\n",
        "#     print(\"\\nQUESTION:\", r[\"original\"])\n",
        "#     print(\"CLEAN:\", r[\"clean\"])\n",
        "#     print(\"ID MAP:\", r[\"id_map\"])\n",
        "#     print(\"SQL (placeholders):\", r[\"sql_placeholders\"])\n",
        "#     print(\"VALID:\", r[\"valid\"])\n",
        "#     if r[\"valid\"]:\n",
        "#         print(\"SQL (final):\", r[\"sql_final\"])\n",
        "#     else:\n",
        "#         print(\"ERRORS:\", r[\"errors\"])\n",
        "#         print(\"COMPLETION (preview):\", r[\"completion\"][:300])\n",
        "\n",
        "# while True:\n",
        "#     q = input(\"\\nAsk a question (blank to stop): \").strip()\n",
        "#     if not q:\n",
        "#         break\n",
        "#     ask(q)\n"
      ],
      "metadata": {
        "id": "shUsb-8vkjJL"
      },
      "id": "shUsb-8vkjJL",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Compact the trained model (fp16 + optional int8 dynamic quantization) ===\n",
        "# fp16: smaller + fast on GPU (use model.half()).\n",
        "# int8 dynamic: much smaller + faster on CPU (Linear layers quantized).\n",
        "\n",
        "import os, torch\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR = \"/content/compact_sql_agent\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def file_mb(p):\n",
        "    return Path(p).stat().st_size / (1024**2)\n",
        "\n",
        "# 1) Save FP16 checkpoint (best for GPU inference)\n",
        "fp16_path = os.path.join(OUT_DIR, \"model_fp16.pt\")\n",
        "model_fp16 = model.to(\"cpu\").eval().half()\n",
        "torch.save({\"model_state_dict\": model_fp16.state_dict(), \"vocab_size\": tokenizer.get_vocab_size()}, fp16_path)\n",
        "print(\"Saved FP16:\", fp16_path, f\"({file_mb(fp16_path):.2f} MB)\")\n",
        "\n",
        "# 2) Save INT8 dynamic-quantized checkpoint (best for CPU inference)\n",
        "# Note: runs on CPU; do NOT move this quantized model to CUDA.\n",
        "from torch.ao.quantization import quantize_dynamic\n",
        "\n",
        "int8_path = os.path.join(OUT_DIR, \"model_int8_dynamic.pt\")\n",
        "model_fp32 = model.to(\"cpu\").eval().float()\n",
        "model_int8 = quantize_dynamic(model_fp32, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "torch.save({\"model_state_dict\": model_int8.state_dict(), \"vocab_size\": tokenizer.get_vocab_size()}, int8_path)\n",
        "print(\"Saved INT8 dynamic:\", int8_path, f\"({file_mb(int8_path):.2f} MB)\")\n",
        "\n",
        "print(\"\\nHow to use:\")\n",
        "print(\"- GPU: load fp16 state_dict into GPTSmall(...).half().to('cuda')\")\n",
        "print(\"- CPU: load int8 state_dict into quantized model (keep on CPU)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de4QYGqEliTd",
        "outputId": "c6fd37cd-aa1a-46c2-c8bb-b48412a30726"
      },
      "id": "de4QYGqEliTd",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved FP16: /content/compact_sql_agent/model_fp16.pt (28.83 MB)\n",
            "Saved INT8 dynamic: /content/compact_sql_agent/model_int8_dynamic.pt (29.56 MB)\n",
            "\n",
            "How to use:\n",
            "- GPU: load fp16 state_dict into GPTSmall(...).half().to('cuda')\n",
            "- CPU: load int8 state_dict into quantized model (keep on CPU)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1390703064.py:26: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_int8 = quantize_dynamic(model_fp32, {torch.nn.Linear}, dtype=torch.qint8)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}