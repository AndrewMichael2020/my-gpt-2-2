{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare SQL Agent - Model-First MVP\n",
    "## End-to-End Training and Inference Pipeline\n",
    "\n",
    "This notebook implements a complete pipeline:\n",
    "1. Generate synthetic T-SQL training dataset\n",
    "2. Train a BPE tokenizer\n",
    "3. Train a tiny decoder-only model from scratch\n",
    "4. Run inference with validation\n",
    "5. Demo with 3 example questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers tokenizers datasets tqdm jsonlines safetensors accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('artifacts/tokenizer', exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example healthcare schema\n",
    "EXAMPLE_SCHEMA = {\n",
    "    \"schema_name\": \"healthcare_analytics\",\n",
    "    \"tables\": [\n",
    "        {\n",
    "            \"name\": \"Patients\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"PatientID\", \"type\": \"INT\", \"pk\": True},\n",
    "                {\"name\": \"FirstName\", \"type\": \"VARCHAR(50)\"},\n",
    "                {\"name\": \"LastName\", \"type\": \"VARCHAR(50)\"},\n",
    "                {\"name\": \"DateOfBirth\", \"type\": \"DATE\"},\n",
    "                {\"name\": \"Gender\", \"type\": \"VARCHAR(10)\"},\n",
    "                {\"name\": \"InsuranceProvider\", \"type\": \"VARCHAR(100)\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Visits\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"VisitID\", \"type\": \"INT\", \"pk\": True},\n",
    "                {\"name\": \"PatientID\", \"type\": \"INT\", \"fk\": \"Patients.PatientID\"},\n",
    "                {\"name\": \"VisitDate\", \"type\": \"DATE\"},\n",
    "                {\"name\": \"DepartmentID\", \"type\": \"INT\"},\n",
    "                {\"name\": \"ProviderID\", \"type\": \"INT\"},\n",
    "                {\"name\": \"VisitType\", \"type\": \"VARCHAR(50)\"},\n",
    "                {\"name\": \"TotalCharge\", \"type\": \"DECIMAL(10,2)\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Departments\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"DepartmentID\", \"type\": \"INT\", \"pk\": True},\n",
    "                {\"name\": \"DepartmentName\", \"type\": \"VARCHAR(100)\"},\n",
    "                {\"name\": \"Location\", \"type\": \"VARCHAR(100)\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Providers\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"ProviderID\", \"type\": \"INT\", \"pk\": True},\n",
    "                {\"name\": \"ProviderName\", \"type\": \"VARCHAR(100)\"},\n",
    "                {\"name\": \"Specialty\", \"type\": \"VARCHAR(100)\"},\n",
    "                {\"name\": \"DepartmentID\", \"type\": \"INT\", \"fk\": \"Departments.DepartmentID\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Diagnoses\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"DiagnosisID\", \"type\": \"INT\", \"pk\": True},\n",
    "                {\"name\": \"VisitID\", \"type\": \"INT\", \"fk\": \"Visits.VisitID\"},\n",
    "                {\"name\": \"ICDCode\", \"type\": \"VARCHAR(20)\"},\n",
    "                {\"name\": \"DiagnosisDescription\", \"type\": \"VARCHAR(255)\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save schema\n",
    "with open('data/example_schema.json', 'w') as f:\n",
    "    json.dump(EXAMPLE_SCHEMA, f, indent=2)\n",
    "\n",
    "print(\"Schema created with\", len(EXAMPLE_SCHEMA['tables']), \"tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_schema_compact_text(schema: Dict) -> str:\n",
    "    \"\"\"Generate compact schema text for prompts.\"\"\"\n",
    "    lines = []\n",
    "    for table in schema['tables']:\n",
    "        cols = ', '.join([c['name'] for c in table['columns']])\n",
    "        lines.append(f\"{table['name']}({cols})\")\n",
    "    return '; '.join(lines)\n",
    "\n",
    "def extract_placeholders(question: str) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"Extract IDs and dates, replace with placeholders.\"\"\"\n",
    "    id_map = {}\n",
    "    counter = 1\n",
    "    \n",
    "    # Extract numeric IDs (pattern: number after 'ID', 'id', or standalone numbers in context)\n",
    "    def replace_id(match):\n",
    "        nonlocal counter\n",
    "        value = match.group(0)\n",
    "        placeholder = f\"__ID_{counter}__\"\n",
    "        id_map[placeholder] = value\n",
    "        counter += 1\n",
    "        return placeholder\n",
    "    \n",
    "    # Replace numeric patterns (simple heuristic for MVP)\n",
    "    question_clean = re.sub(r'\\b\\d{4,}\\b', replace_id, question)\n",
    "    \n",
    "    # Extract dates\n",
    "    date_counter = 1\n",
    "    def replace_date(match):\n",
    "        nonlocal date_counter\n",
    "        value = match.group(0)\n",
    "        placeholder = f\"__DATE_{date_counter}__\"\n",
    "        id_map[placeholder] = value\n",
    "        date_counter += 1\n",
    "        return placeholder\n",
    "    \n",
    "    question_clean = re.sub(r'\\d{4}-\\d{2}-\\d{2}', replace_date, question_clean)\n",
    "    \n",
    "    return question_clean, id_map\n",
    "\n",
    "def generate_dataset_samples(schema: Dict, num_samples: int) -> List[Dict]:\n",
    "    \"\"\"Generate synthetic SQL training samples.\"\"\"\n",
    "    samples = []\n",
    "    schema_text = generate_schema_compact_text(schema)\n",
    "    \n",
    "    templates = [\n",
    "        # COUNT templates\n",
    "        {\n",
    "            \"question\": \"How many visits did patient {patient_id} have in department {dept_id}?\",\n",
    "            \"sql\": \"SELECT COUNT(*) FROM Visits WHERE PatientID = {patient_id_ph} AND DepartmentID = {dept_id_ph};\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Count total visits in {year}\",\n",
    "            \"sql\": \"SELECT COUNT(*) FROM Visits WHERE YEAR(VisitDate) = {year_ph};\"\n",
    "        },\n",
    "        # SUM templates\n",
    "        {\n",
    "            \"question\": \"What is the total charge for patient {patient_id}?\",\n",
    "            \"sql\": \"SELECT SUM(TotalCharge) FROM Visits WHERE PatientID = {patient_id_ph};\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Total charges for department {dept_id} in {year}\",\n",
    "            \"sql\": \"SELECT SUM(TotalCharge) FROM Visits WHERE DepartmentID = {dept_id_ph} AND YEAR(VisitDate) = {year_ph};\"\n",
    "        },\n",
    "        # AVG templates\n",
    "        {\n",
    "            \"question\": \"What is the average charge per visit for provider {provider_id}?\",\n",
    "            \"sql\": \"SELECT AVG(TotalCharge) FROM Visits WHERE ProviderID = {provider_id_ph};\"\n",
    "        },\n",
    "        # GROUP BY templates\n",
    "        {\n",
    "            \"question\": \"Show visit counts by department for patient {patient_id}\",\n",
    "            \"sql\": \"SELECT DepartmentID, COUNT(*) FROM Visits WHERE PatientID = {patient_id_ph} GROUP BY DepartmentID;\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Show monthly visit counts in {year}\",\n",
    "            \"sql\": \"SELECT MONTH(VisitDate) AS Month, COUNT(*) FROM Visits WHERE YEAR(VisitDate) = {year_ph} GROUP BY MONTH(VisitDate);\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Total charges by provider in department {dept_id}\",\n",
    "            \"sql\": \"SELECT ProviderID, SUM(TotalCharge) FROM Visits WHERE DepartmentID = {dept_id_ph} GROUP BY ProviderID;\"\n",
    "        },\n",
    "        # JOIN templates\n",
    "        {\n",
    "            \"question\": \"List all visits with patient names for patient {patient_id}\",\n",
    "            \"sql\": \"SELECT V.VisitID, P.FirstName, P.LastName, V.VisitDate FROM Visits V JOIN Patients P ON V.PatientID = P.PatientID WHERE V.PatientID = {patient_id_ph};\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Show visit counts by department name\",\n",
    "            \"sql\": \"SELECT D.DepartmentName, COUNT(V.VisitID) FROM Visits V JOIN Departments D ON V.DepartmentID = D.DepartmentID GROUP BY D.DepartmentName;\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        # Generate random IDs\n",
    "        patient_id = random.randint(1000, 9999)\n",
    "        dept_id = random.randint(10, 99)\n",
    "        provider_id = random.randint(100, 999)\n",
    "        year = random.randint(2020, 2024)\n",
    "        \n",
    "        # Fill template\n",
    "        question = template['question'].format(\n",
    "            patient_id=patient_id,\n",
    "            dept_id=dept_id,\n",
    "            provider_id=provider_id,\n",
    "            year=year\n",
    "        )\n",
    "        \n",
    "        # Extract placeholders\n",
    "        question_clean, id_map = extract_placeholders(question)\n",
    "        \n",
    "        # Build SQL with placeholders\n",
    "        sql = template['sql'].format(\n",
    "            patient_id_ph=\"__ID_1__\" if '{patient_id_ph}' in template['sql'] else '',\n",
    "            dept_id_ph=\"__ID_2__\" if '{dept_id_ph}' in template['sql'] else \"__ID_1__\",\n",
    "            provider_id_ph=\"__ID_1__\",\n",
    "            year_ph=\"__ID_1__\" if '{year_ph}' in template['sql'] else \"__ID_2__\"\n",
    "        )\n",
    "        \n",
    "        # Create ID map text\n",
    "        id_map_text = ', '.join([f\"{k}={v}\" for k, v in id_map.items()]) if id_map else \"None\"\n",
    "        \n",
    "        # Create training record\n",
    "        record = {\n",
    "            \"schema_id\": schema['schema_name'],\n",
    "            \"schema_text\": schema_text,\n",
    "            \"question\": question_clean,\n",
    "            \"id_map\": id_map_text,\n",
    "            \"sql\": sql\n",
    "        }\n",
    "        \n",
    "        samples.append(record)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating training dataset...\")\n",
    "train_samples = generate_dataset_samples(EXAMPLE_SCHEMA, 5000)\n",
    "\n",
    "print(\"Generating validation dataset...\")\n",
    "val_samples = generate_dataset_samples(EXAMPLE_SCHEMA, 200)\n",
    "\n",
    "# Save datasets\n",
    "with open('data/train.jsonl', 'w') as f:\n",
    "    for sample in train_samples:\n",
    "        f.write(json.dumps(sample) + '\\n')\n",
    "\n",
    "with open('data/val.jsonl', 'w') as f:\n",
    "    for sample in val_samples:\n",
    "        f.write(json.dumps(sample) + '\\n')\n",
    "\n",
    "print(f\"Generated {len(train_samples)} training samples\")\n",
    "print(f\"Generated {len(val_samples)} validation samples\")\n",
    "print(\"\\nExample sample:\")\n",
    "print(json.dumps(train_samples[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_text(sample: Dict) -> str:\n",
    "    \"\"\"Format sample into training text with markers.\"\"\"\n",
    "    return f\"\"\"SCHEMA: {sample['schema_text']}\n",
    "QUESTION: {sample['question']}\n",
    "ID_MAP: {sample['id_map']}\n",
    "SQL: {sample['sql']}\"\"\"\n",
    "\n",
    "# Prepare training text for tokenizer\n",
    "print(\"Preparing tokenizer training data...\")\n",
    "tokenizer_training_file = 'data/tokenizer_train.txt'\n",
    "with open(tokenizer_training_file, 'w') as f:\n",
    "    for sample in train_samples:\n",
    "        f.write(format_training_text(sample) + '\\n\\n')\n",
    "\n",
    "# Train tokenizer\n",
    "print(\"Training BPE tokenizer...\")\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    ")\n",
    "\n",
    "tokenizer.train([tokenizer_training_file], trainer)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save('artifacts/tokenizer/tokenizer.json')\n",
    "print(\"Tokenizer trained and saved!\")\n",
    "print(f\"Vocab size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "# Test tokenizer\n",
    "test_text = \"SELECT COUNT(*) FROM Visits WHERE PatientID = __ID_1__;\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(f\"\\nTest encoding: {test_text}\")\n",
    "print(f\"Tokens: {encoded.tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Decoder-Only Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attention(x, x, x, attn_mask=mask, need_weights=False)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        # FFN with residual\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int = 512, n_layers: int = 8, \n",
    "                 n_heads: int = 8, max_seq_len: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1)\n",
    "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))\n",
    "        \n",
    "        # Embeddings\n",
    "        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask=causal_mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        logits = self.output(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Model config (tiny for MVP - ~50M params)\n",
    "MODEL_CONFIG = {\n",
    "    'vocab_size': tokenizer.get_vocab_size(),\n",
    "    'd_model': 512,\n",
    "    'n_layers': 8,\n",
    "    'n_heads': 8,\n",
    "    'max_seq_len': 512,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DecoderOnlyTransformer(**MODEL_CONFIG).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {total_params:,} parameters ({total_params/1e6:.1f}M)\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, tokenizer: Tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "        \n",
    "        # Load samples\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            for line in f:\n",
    "                self.samples.append(json.loads(line))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Format text\n",
    "        text = format_training_text(sample)\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        tokens = encoded.ids\n",
    "        \n",
    "        # Truncate or pad\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens = tokens + [0] * (self.max_length - len(tokens))  # 0 is [PAD]\n",
    "        \n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SQLDataset('data/train.jsonl', tokenizer, max_length=MODEL_CONFIG['max_seq_len'])\n",
    "val_dataset = SQLDataset('data/val.jsonl', tokenizer, max_length=MODEL_CONFIG['max_seq_len'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch_idx, input_ids in enumerate(pbar):\n",
    "        input_ids = input_ids.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss_fct = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids in val_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = input_ids[:, 1:].contiguous()\n",
    "            \n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
    "num_epochs = 3  # Short training for MVP\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    val_loss = validate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'model_config': MODEL_CONFIG\n",
    "    }\n",
    "    torch.save(checkpoint, f'checkpoints/checkpoint_epoch_{epoch}.pt')\n",
    "    print(f\"Checkpoint saved: checkpoint_epoch_{epoch}.pt\\n\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(model, tokenizer, schema_text, question, id_map_text, \n",
    "                 max_length=256, temperature=0.1, device='cuda'):\n",
    "    \"\"\"Generate SQL from a question using the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = f\"\"\"SCHEMA: {schema_text}\n",
    "QUESTION: {question}\n",
    "ID_MAP: {id_map_text}\n",
    "SQL: \"\"\"\n",
    "    \n",
    "    # Encode prompt\n",
    "    encoded = tokenizer.encode(prompt)\n",
    "    input_ids = torch.tensor([encoded.ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    generated_ids = input_ids[0].tolist()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get current sequence\n",
    "            current_ids = torch.tensor([generated_ids], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(current_ids)\n",
    "            \n",
    "            # Get next token logits\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Greedy sampling\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            \n",
    "            # Add to sequence\n",
    "            generated_ids.append(next_token_id)\n",
    "            \n",
    "            # Check for EOS or semicolon\n",
    "            decoded_text = tokenizer.decode(generated_ids)\n",
    "            if ';' in decoded_text[len(prompt):] or next_token_id == 3:  # 3 is [EOS]\n",
    "                break\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(generated_ids)\n",
    "    \n",
    "    # Extract SQL part\n",
    "    if 'SQL:' in generated_text:\n",
    "        sql_part = generated_text.split('SQL:')[1].strip()\n",
    "        # Clean up\n",
    "        if ';' in sql_part:\n",
    "            sql_part = sql_part.split(';')[0] + ';'\n",
    "        return sql_part\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "print(\"Inference function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Validation Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sql(sql: str, schema: Dict, id_map: Dict[str, str]) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"Validate generated SQL against strict rules.\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Rule 1: Must be exactly one statement\n",
    "    if sql.count(';') != 1:\n",
    "        errors.append(\"Must contain exactly one statement ending with ';'\")\n",
    "    \n",
    "    # Rule 2: Must end with semicolon\n",
    "    if not sql.strip().endswith(';'):\n",
    "        errors.append(\"Must end with ';'\")\n",
    "    \n",
    "    # Rule 3: Must be SELECT only\n",
    "    sql_upper = sql.upper()\n",
    "    if not sql_upper.strip().startswith('SELECT'):\n",
    "        errors.append(\"Must start with SELECT\")\n",
    "    \n",
    "    # Rule 4: Block DML/DDL keywords\n",
    "    forbidden_keywords = ['INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER', 'TRUNCATE', 'EXEC', 'EXECUTE']\n",
    "    for keyword in forbidden_keywords:\n",
    "        if keyword in sql_upper:\n",
    "            errors.append(f\"Forbidden keyword: {keyword}\")\n",
    "    \n",
    "    # Rule 5: Schema-known tables only (basic check)\n",
    "    table_names = [t['name'] for t in schema['tables']]\n",
    "    for table in table_names:\n",
    "        if table.upper() in sql_upper:\n",
    "            # Found at least one known table\n",
    "            break\n",
    "    else:\n",
    "        errors.append(\"No known tables found in SQL\")\n",
    "    \n",
    "    # Rule 6: Placeholder integrity\n",
    "    placeholders_in_sql = re.findall(r'__ID_\\d+__|__DATE_\\d+__', sql)\n",
    "    for ph in placeholders_in_sql:\n",
    "        if ph not in id_map:\n",
    "            errors.append(f\"Unknown placeholder: {ph}\")\n",
    "    \n",
    "    return len(errors) == 0, errors\n",
    "\n",
    "def reinject_ids(sql: str, id_map: Dict[str, str]) -> str:\n",
    "    \"\"\"Replace placeholders with actual IDs.\"\"\"\n",
    "    for placeholder, value in id_map.items():\n",
    "        sql = sql.replace(placeholder, value)\n",
    "    return sql\n",
    "\n",
    "print(\"Validation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. End-to-End Demo with 3 Example Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest checkpoint\n",
    "latest_checkpoint = 'checkpoints/checkpoint_epoch_3.pt'\n",
    "if os.path.exists(latest_checkpoint):\n",
    "    checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\\n\")\n",
    "else:\n",
    "    print(\"No checkpoint found, using untrained model\\n\")\n",
    "\n",
    "schema_text = generate_schema_compact_text(EXAMPLE_SCHEMA)\n",
    "\n",
    "# Example questions\n",
    "test_questions = [\n",
    "    \"How many visits did patient 5432 have in department 25?\",\n",
    "    \"What is the total charge for patient 7890?\",\n",
    "    \"Show monthly visit counts in 2023\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING SQL FOR TEST QUESTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "passed = 0\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n[Question {i}]\")\n",
    "    print(f\"Original: {question}\")\n",
    "    \n",
    "    # Extract placeholders\n",
    "    question_clean, id_map = extract_placeholders(question)\n",
    "    print(f\"Clean: {question_clean}\")\n",
    "    print(f\"ID Map: {id_map}\")\n",
    "    \n",
    "    # Generate SQL\n",
    "    id_map_text = ', '.join([f\"{k}={v}\" for k, v in id_map.items()]) if id_map else \"None\"\n",
    "    generated_sql = generate_sql(model, tokenizer, schema_text, question_clean, id_map_text, device=device)\n",
    "    \n",
    "    print(f\"\\nGenerated SQL (with placeholders):\")\n",
    "    print(f\"  {generated_sql}\")\n",
    "    \n",
    "    # Validate\n",
    "    is_valid, errors = validate_sql(generated_sql, EXAMPLE_SCHEMA, id_map)\n",
    "    \n",
    "    if is_valid:\n",
    "        print(\"✓ Validation: PASSED\")\n",
    "        passed += 1\n",
    "        \n",
    "        # Reinject IDs\n",
    "        final_sql = reinject_ids(generated_sql, id_map)\n",
    "        print(f\"\\nFinal SQL (with real IDs):\")\n",
    "        print(f\"  {final_sql}\")\n",
    "    else:\n",
    "        print(\"✗ Validation: FAILED\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"RESULTS: {passed}/3 questions passed validation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if passed >= 2:\n",
    "    print(\"\\n✓ SUCCESS: MVP acceptance criteria met (>=2/3 passed)\")\n",
    "else:\n",
    "    print(\"\\n✗ INCOMPLETE: Need more training or better templates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\\n\n",
    "=================================================================================\n",
    "MVP PIPELINE COMPLETE!\n",
    "=================================================================================\n",
    "\n",
    "What we built:\n",
    "✓ Example healthcare schema with 5 tables\n",
    "✓ Dataset generator with 5,000+ training and 200+ validation samples\n",
    "✓ BPE tokenizer trained on SQL corpus (vocab size: 8,000)\n",
    "✓ Tiny decoder-only transformer (~50M parameters)\n",
    "✓ Training loop with checkpointing\n",
    "✓ Inference with prompt formatting\n",
    "✓ Validation gates (SELECT-only, schema checks, placeholders)\n",
    "✓ ID placeholder vault and reinjection\n",
    "✓ End-to-end demo with 3 test questions\n",
    "\n",
    "Files created:\n",
    "- data/example_schema.json\n",
    "- data/train.jsonl (5,000 samples)\n",
    "- data/val.jsonl (200 samples)\n",
    "- artifacts/tokenizer/tokenizer.json\n",
    "- checkpoints/checkpoint_epoch_*.pt\n",
    "\n",
    "Next steps to scale:\n",
    "1. Increase dataset size to 200k-800k samples\n",
    "2. Add more SQL template diversity (JOINs, subqueries, HAVING, etc.)\n",
    "3. Scale model to ~300M parameters\n",
    "4. Train for more epochs with learning rate scheduling\n",
    "5. Add evaluation harness with held-out test set\n",
    "6. Fine-tune hyperparameters (learning rate, batch size, etc.)\n",
    "7. Implement beam search or nucleus sampling for generation\n",
    "8. Add more sophisticated schema validation\n",
    "\n",
    "=================================================================================\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
