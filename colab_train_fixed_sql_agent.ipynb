{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Healthcare SQL Agent (Model-First) \u2014 Colab Training Notebook (Fixed)\n",
        "\n",
        "This notebook trains a small decoder-only model **from scratch** to generate **one T-SQL SELECT** statement from a schema-aware question.\n",
        "\n",
        "Key fixes:\n",
        "- Deterministic ID vault for all identifiers\n",
        "- Hard `</SQL>` sentinel\n",
        "- Loss masking by construction (prompt masked, SQL supervised)\n",
        "- Token-based completion slicing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 0) Install dependencies\n",
        "!pip -q install tokenizers==0.15.2\n",
        "\n",
        "import os, re, json, time, random\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 1) Repro + Config\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "DATA_DIR = \"/content/data_sql_agent\"\n",
        "ART_DIR  = \"/content/artifacts_sql_agent\"\n",
        "CKPT_DIR = \"/content/checkpoints_sql_agent\"\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(ART_DIR, exist_ok=True)\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "TRAIN_JSONL = os.path.join(DATA_DIR, \"train.jsonl\")\n",
        "VAL_JSONL   = os.path.join(DATA_DIR, \"val.jsonl\")\n",
        "TOKENIZER_PATH = os.path.join(ART_DIR, \"tokenizer.json\")\n",
        "\n",
        "N_TRAIN = 5000\n",
        "N_VAL   = 200\n",
        "\n",
        "MAX_LEN = 384\n",
        "BATCH_SIZE = 8\n",
        "LR = 2e-4\n",
        "EPOCHS = 3\n",
        "WARMUP_STEPS = 200\n",
        "GRAD_CLIP = 1.0\n",
        "\n",
        "D_MODEL = 384\n",
        "N_HEAD  = 6\n",
        "N_LAYER = 8\n",
        "D_FF    = 4 * D_MODEL\n",
        "DROPOUT = 0.1\n",
        "\n",
        "GEN_MAX_NEW_TOKENS = 160\n",
        "TEMPERATURE = 0.0\n",
        "\n",
        "SQL_END = \"</SQL>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 2) Schema + Templates\n",
        "SCHEMA_ID = \"healthcare_analytics\"\n",
        "SCHEMA_TEXT = (\n",
        "    \"Patients ( PatientID, FirstName, LastName, DateOfBirth, Gender, InsuranceProvider ); \"\n",
        "    \"Visits ( VisitID, PatientID, VisitDate, DepartmentID, ProviderID, VisitType, TotalCharge ); \"\n",
        "    \"Departments ( DepartmentID, DepartmentName, Location ); \"\n",
        "    \"Providers ( ProviderID, ProviderName, Specialty, DepartmentID ); \"\n",
        "    \"Diagnoses ( DiagnosisID, VisitID, ICDCode, DiagnosisDescription )\"\n",
        ")\n",
        "\n",
        "QUESTION_TEMPLATES = [\n",
        "    (\"How many visits did patient {PID} have in department {DID}?\",\n",
        "     \"SELECT COUNT(*) FROM Visits WHERE PatientID = {PID} AND DepartmentID = {DID};\"),\n",
        "    (\"What is the total charge for patient {PID}?\",\n",
        "     \"SELECT SUM(TotalCharge) FROM Visits WHERE PatientID = {PID};\"),\n",
        "    (\"Show monthly visit counts in {YEAR}\",\n",
        "     \"SELECT MONTH(VisitDate) AS [Month], COUNT(*) AS VisitCount \"\n",
        "     \"FROM Visits WHERE YEAR(VisitDate) = {YEAR} GROUP BY MONTH(VisitDate) ORDER BY [Month];\"),\n",
        "    (\"List providers in department {DID} ordered by name\",\n",
        "     \"SELECT ProviderID, ProviderName FROM Providers WHERE DepartmentID = {DID} ORDER BY ProviderName;\"),\n",
        "    (\"Count visits by department in {YEAR}\",\n",
        "     \"SELECT DepartmentID, COUNT(*) AS VisitCount FROM Visits \"\n",
        "     \"WHERE YEAR(VisitDate) = {YEAR} GROUP BY DepartmentID ORDER BY VisitCount DESC;\"),\n",
        "]\n",
        "\n",
        "def rand_id(min_v: int, max_v: int) -> int:\n",
        "    return random.randint(min_v, max_v)\n",
        "\n",
        "def rand_year() -> int:\n",
        "    return random.randint(2018, 2024)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 3) ID Vault (Aggressive, Deterministic)\n",
        "def extract_placeholders(question: str) -> Tuple[str, Dict[str, str]]:\n",
        "    id_map: Dict[str, str] = {}\n",
        "    id_counter = 1\n",
        "    date_counter = 1\n",
        "    year_counter = 1\n",
        "    q = question\n",
        "\n",
        "    date_pat = re.compile(r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\")\n",
        "\n",
        "    def repl_date(m):\n",
        "        nonlocal date_counter\n",
        "        value = m.group(0)\n",
        "        ph = f\"__DATE_{date_counter}__\"\n",
        "        id_map[ph] = value\n",
        "        date_counter += 1\n",
        "        return ph\n",
        "\n",
        "    q = date_pat.sub(repl_date, q)\n",
        "\n",
        "    int_pat = re.compile(r\"\\b\\d{1,6}\\b\")\n",
        "\n",
        "    def looks_like_year(num_str: str, text: str, start_idx: int) -> bool:\n",
        "        try:\n",
        "            n = int(num_str)\n",
        "        except:\n",
        "            return False\n",
        "        if not (1900 <= n <= 2100):\n",
        "            return False\n",
        "        left = max(0, start_idx - 18)\n",
        "        window = text[left:start_idx].lower()\n",
        "        triggers = [\" in \", \" year\", \" during\", \" for \", \" within\", \" by \"]\n",
        "        return any(t in window for t in triggers)\n",
        "\n",
        "    def repl_int(m):\n",
        "        nonlocal id_counter, year_counter\n",
        "        value = m.group(0)\n",
        "        start = m.start()\n",
        "        if looks_like_year(value, q, start):\n",
        "            ph = f\"__YEAR_{year_counter}__\"\n",
        "            id_map[ph] = value\n",
        "            year_counter += 1\n",
        "            return ph\n",
        "        ph = f\"__ID_{id_counter}__\"\n",
        "        id_map[ph] = value\n",
        "        id_counter += 1\n",
        "        return ph\n",
        "\n",
        "    q = int_pat.sub(repl_int, q)\n",
        "    return q, id_map\n",
        "\n",
        "def id_map_to_text(id_map: Dict[str, str]) -> str:\n",
        "    def idx(ph: str):\n",
        "        m = re.search(r\"_(\\d+)__\", ph)\n",
        "        return int(m.group(1)) if m else 0\n",
        "    items = sorted(id_map.items(), key=lambda kv: (kv[0].split(\"_\")[1], idx(kv[0])))\n",
        "    return \" ; \".join([f\"{k} = {v}\" for k, v in items]) if items else \"\"\n",
        "\n",
        "def finalize_sql(sql: str) -> str:\n",
        "    sql = sql.strip()\n",
        "    if not sql.endswith(\";\"):\n",
        "        sql = sql.rstrip(\";\") + \";\"\n",
        "    return sql + \" \" + SQL_END\n",
        "\n",
        "q0 = \"How many visits did patient 5432 have in department 25?\"\n",
        "clean0, map0 = extract_placeholders(q0)\n",
        "print(\"Original:\", q0)\n",
        "print(\"Clean:\", clean0)\n",
        "print(\"ID Map:\", map0)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 4) Generate dataset (JSONL)\n",
        "def render_template(q_tmpl: str, sql_tmpl: str) -> str:\n",
        "    pid = str(rand_id(1000, 9999))\n",
        "    did = str(rand_id(1, 99))\n",
        "    yr  = str(rand_year())\n",
        "\n",
        "    q = q_tmpl.format(PID=pid, DID=did, YEAR=yr)\n",
        "    sql = sql_tmpl.format(PID=pid, DID=did, YEAR=yr)\n",
        "\n",
        "    clean_q, id_map = extract_placeholders(q)\n",
        "\n",
        "    val_to_ph = {}\n",
        "    for ph, val in id_map.items():\n",
        "        val_to_ph.setdefault(val, ph)\n",
        "\n",
        "    sql_ph = sql\n",
        "    for val, ph in val_to_ph.items():\n",
        "        sql_ph = re.sub(rf\"\\b{re.escape(val)}\\b\", ph, sql_ph)\n",
        "\n",
        "    sample = {\n",
        "        \"schema_id\": SCHEMA_ID,\n",
        "        \"schema_text\": SCHEMA_TEXT,\n",
        "        \"question\": clean_q,\n",
        "        \"id_map\": id_map_to_text(id_map),\n",
        "        \"sql\": finalize_sql(sql_ph),\n",
        "    }\n",
        "    return json.dumps(sample, ensure_ascii=False)\n",
        "\n",
        "def write_jsonl(path: str, n: int):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for _ in range(n):\n",
        "            q_tmpl, sql_tmpl = random.choice(QUESTION_TEMPLATES)\n",
        "            f.write(render_template(q_tmpl, sql_tmpl) + \"\\n\")\n",
        "\n",
        "print(\"Generating training dataset...\")\n",
        "write_jsonl(TRAIN_JSONL, N_TRAIN)\n",
        "print(\"Generating validation dataset...\")\n",
        "write_jsonl(VAL_JSONL, N_VAL)\n",
        "\n",
        "print(f\"Generated {N_TRAIN} training samples\")\n",
        "print(f\"Generated {N_VAL} validation samples\")\n",
        "\n",
        "with open(TRAIN_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
        "    ex = json.loads(next(f))\n",
        "print(\"Example sample:\")\n",
        "print(json.dumps(ex, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 5) Tokenizer training (BPE) with special tokens\n",
        "def format_training_text(sample: Dict) -> str:\n",
        "    return (\n",
        "        f\"SCHEMA: {sample['schema_text']}\\n\"\n",
        "        f\"QUESTION: {sample['question']}\\n\"\n",
        "        f\"ID_MAP: {sample['id_map']}\\n\"\n",
        "        f\"SQL: {sample['sql']}\"\n",
        "    )\n",
        "\n",
        "def iter_training_texts(jsonl_path: str):\n",
        "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            s = json.loads(line)\n",
        "            yield format_training_text(s)\n",
        "\n",
        "special_tokens = [\n",
        "    \"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\",\n",
        "    \"SCHEMA:\", \"QUESTION:\", \"ID_MAP:\", \"SQL:\",\n",
        "    SQL_END,\n",
        "    \"__ID_\", \"__DATE_\", \"__YEAR_\",\n",
        "]\n",
        "for i in range(1, 65):\n",
        "    special_tokens.append(f\"__ID_{i}__\")\n",
        "for i in range(1, 17):\n",
        "    special_tokens.append(f\"__DATE_{i}__\")\n",
        "for i in range(1, 9):\n",
        "    special_tokens.append(f\"__YEAR_{i}__\")\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = BpeTrainer(vocab_size=2000, min_frequency=2, special_tokens=special_tokens, show_progress=True)\n",
        "\n",
        "texts = list(iter_training_texts(TRAIN_JSONL))\n",
        "print(f\"Training tokenizer with {len(special_tokens)} special tokens...\")\n",
        "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
        "tokenizer.save(TOKENIZER_PATH)\n",
        "print(\"Tokenizer saved:\", TOKENIZER_PATH)\n",
        "print(\"Vocab size:\", tokenizer.get_vocab_size())\n",
        "\n",
        "t1 = \"SELECT COUNT(*) FROM Visits WHERE PatientID = __ID_1__;\"\n",
        "print(\"Tokens:\", tokenizer.encode(t1).tokens)\n",
        "\n",
        "t2 = \"SQL: SELECT * FROM Visits; </SQL>\"\n",
        "print(\"Tokens:\", tokenizer.encode(t2).tokens)\n",
        "print(\"SQL: tokens:\", tokenizer.encode(\"SQL:\").tokens, tokenizer.encode(\"SQL:\").ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 6) Dataset with loss masking BY CONSTRUCTION\n",
        "PAD_ID = tokenizer.token_to_id(\"[PAD]\")\n",
        "SQL_END_ID = tokenizer.encode(SQL_END).ids[0]\n",
        "assert PAD_ID is not None and SQL_END_ID is not None\n",
        "\n",
        "def build_prompt_text(sample: Dict) -> str:\n",
        "    return (\n",
        "        f\"SCHEMA: {sample['schema_text']}\\n\"\n",
        "        f\"QUESTION: {sample['question']}\\n\"\n",
        "        f\"ID_MAP: {sample['id_map']}\\n\"\n",
        "        f\"SQL:\"\n",
        "    )\n",
        "\n",
        "class SQLDataset(Dataset):\n",
        "    def __init__(self, jsonl_path: str, tokenizer: Tokenizer, max_len: int):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.samples = []\n",
        "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                self.samples.append(json.loads(line))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        prompt = build_prompt_text(s)\n",
        "        sql = \" \" + s[\"sql\"].strip()\n",
        "\n",
        "        prompt_ids = self.tokenizer.encode(prompt).ids\n",
        "        sql_ids = self.tokenizer.encode(sql).ids\n",
        "\n",
        "        input_ids = (prompt_ids + sql_ids)[:self.max_len]\n",
        "        labels = ([-100] * len(prompt_ids) + sql_ids[:])[:self.max_len]\n",
        "\n",
        "        if len(input_ids) < self.max_len:\n",
        "            pad_len = self.max_len - len(input_ids)\n",
        "            input_ids += [PAD_ID] * pad_len\n",
        "            labels += [-100] * pad_len\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "def collate(batch):\n",
        "    x = torch.stack([b[\"input_ids\"] for b in batch], dim=0)\n",
        "    y = torch.stack([b[\"labels\"] for b in batch], dim=0)\n",
        "    return x, y\n",
        "\n",
        "train_ds = SQLDataset(TRAIN_JSONL, tokenizer, MAX_LEN)\n",
        "val_ds   = SQLDataset(VAL_JSONL, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "first = (y[0] != -100).nonzero(as_tuple=True)[0][0].item()\n",
        "print(\"First supervised label idx:\", first)\n",
        "print(\"Decoded near boundary:\", tokenizer.decode(x[0].tolist()[max(0, first-10):first+25]))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 7) Decoder-only model (TransformerEncoder + causal mask)\n",
        "class GPTSmall(nn.Module):\n",
        "    def __init__(self, vocab_size: int, d_model: int, n_head: int, n_layer: int, d_ff: int, dropout: float, max_len: int):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_head,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            activation=\"gelu\",\n",
        "            batch_first=True,\n",
        "            norm_first=True,\n",
        "        )\n",
        "        self.blocks = nn.TransformerEncoder(enc_layer, num_layers=n_layer)\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        B, T = input_ids.shape\n",
        "        pos = torch.arange(0, T, device=input_ids.device).unsqueeze(0).expand(B, T)\n",
        "        x = self.tok_emb(input_ids) + self.pos_emb(pos)\n",
        "        x = self.dropout(x)\n",
        "        causal = torch.triu(torch.ones(T, T, device=input_ids.device, dtype=torch.bool), diagonal=1)\n",
        "        x = self.blocks(x, mask=causal)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "model = GPTSmall(vocab_size, D_MODEL, N_HEAD, N_LAYER, D_FF, DROPOUT, MAX_LEN).to(DEVICE)\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {n_params:,} ({n_params/1e6:.2f}M)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 8) Training loop + checkpointing + warmup\n",
        "def get_lr(step: int, base_lr: float, warmup: int) -> float:\n",
        "    if step < warmup:\n",
        "        return base_lr * (step / max(1, warmup))\n",
        "    return base_lr\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
        "loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "def save_ckpt(epoch: int, global_step: int):\n",
        "    path = os.path.join(CKPT_DIR, f\"checkpoint_epoch_{epoch}.pt\")\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"global_step\": global_step,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }, path)\n",
        "    print(\"Checkpoint saved:\", path)\n",
        "\n",
        "def load_latest_ckpt() -> Tuple[int, int]:\n",
        "    files = [f for f in os.listdir(CKPT_DIR) if f.startswith(\"checkpoint_epoch_\") and f.endswith(\".pt\")]\n",
        "    if not files:\n",
        "        return 0, 0\n",
        "    files.sort(key=lambda x: int(re.search(r\"checkpoint_epoch_(\\d+)\\.pt\", x).group(1)))\n",
        "    latest = files[-1]\n",
        "    ckpt = torch.load(os.path.join(CKPT_DIR, latest), map_location=DEVICE)\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    print(\"Loaded checkpoint:\", latest)\n",
        "    return ckpt[\"epoch\"], ckpt[\"global_step\"]\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loss(loader: DataLoader) -> float:\n",
        "    model.eval()\n",
        "    total = 0.0\n",
        "    n = 0\n",
        "    for input_ids, labels in loader:\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
        "            logits = model(input_ids)\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_labels = labels[:, 1:].contiguous()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        total += loss.item()\n",
        "        n += 1\n",
        "    model.train()\n",
        "    return total / max(1, n)\n",
        "\n",
        "start_epoch, global_step = load_latest_ckpt()\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(start_epoch + 1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    running = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for batch_idx, (input_ids, labels) in enumerate(train_loader, start=1):\n",
        "        global_step += 1\n",
        "        lr_now = get_lr(global_step, LR, WARMUP_STEPS)\n",
        "        for pg in optimizer.param_groups:\n",
        "            pg[\"lr\"] = lr_now\n",
        "\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
        "            logits = model(input_ids)\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_labels = labels[:, 1:].contiguous()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running += loss.item()\n",
        "        n += 1\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch} step {batch_idx}/{len(train_loader)} loss={running/n:.4f} lr={lr_now:.2e}\")\n",
        "\n",
        "    train_loss = running / max(1, n)\n",
        "    val_loss = eval_loss(val_loader)\n",
        "    dt = time.time() - t0\n",
        "    print(f\"\\nEpoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f} (time {dt:.1f}s)\")\n",
        "    save_ckpt(epoch, global_step)\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 9) Inference: token slicing + sentinel stop + extraction + validation\n",
        "def build_prompt(schema_text: str, clean_question: str, id_map_text: str) -> str:\n",
        "    rules = (\n",
        "        \"RULES: Output must be exactly one T-SQL SELECT statement. \"\n",
        "        \"Start with SELECT. End with a semicolon. Use only schema tables/columns. \"\n",
        "        \"Use placeholders exactly as provided. Append </SQL> at the end.\"\n",
        "    )\n",
        "    return (\n",
        "        f\"{rules}\\n\"\n",
        "        f\"SCHEMA: {schema_text}\\n\"\n",
        "        f\"QUESTION: {clean_question}\\n\"\n",
        "        f\"ID_MAP: {id_map_text}\\n\"\n",
        "        f\"SQL:\"\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_ids(prompt_text: str, max_new_tokens: int = GEN_MAX_NEW_TOKENS) -> Tuple[List[int], int]:\n",
        "    model.eval()\n",
        "    prompt_ids = tokenizer.encode(prompt_text).ids\n",
        "    input_ids = torch.tensor([prompt_ids], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if input_ids.size(1) > MAX_LEN:\n",
        "            input_ids = input_ids[:, -MAX_LEN:]\n",
        "        logits = model(input_ids)\n",
        "        next_logits = logits[:, -1, :]\n",
        "\n",
        "        if TEMPERATURE and TEMPERATURE > 0:\n",
        "            probs = torch.softmax(next_logits / TEMPERATURE, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            next_id = torch.argmax(next_logits, dim=-1, keepdim=True)\n",
        "\n",
        "        tok = int(next_id.item())\n",
        "        input_ids = torch.cat([input_ids, next_id], dim=1)\n",
        "\n",
        "        if tok == SQL_END_ID:\n",
        "            break\n",
        "\n",
        "    return input_ids[0].tolist(), len(prompt_ids)\n",
        "\n",
        "def extract_sql(completion_text: str) -> str:\n",
        "    if SQL_END in completion_text:\n",
        "        completion_text = completion_text.split(SQL_END, 1)[0]\n",
        "    m = re.search(r\"\\bSELECT\\b\", completion_text, flags=re.IGNORECASE)\n",
        "    if not m:\n",
        "        return \"\"\n",
        "    tail = completion_text[m.start():]\n",
        "    semi = tail.find(\";\")\n",
        "    if semi == -1:\n",
        "        return \"\"\n",
        "    return tail[:semi+1].strip()\n",
        "\n",
        "DISALLOWED = re.compile(r\"\\b(INSERT|UPDATE|DELETE|MERGE|DROP|ALTER|CREATE|EXEC|GRANT|REVOKE|TRUNCATE)\\b\", re.IGNORECASE)\n",
        "KNOWN_TABLES = {\"Patients\",\"Visits\",\"Departments\",\"Providers\",\"Diagnoses\"}\n",
        "\n",
        "def validate_sql(sql: str, id_map: Dict[str,str]) -> Tuple[bool, List[str]]:\n",
        "    errs = []\n",
        "    s = sql.strip()\n",
        "    if not s:\n",
        "        errs.append(\"No SQL extracted.\")\n",
        "        return False, errs\n",
        "    if not re.match(r\"^SELECT\\b\", s, flags=re.IGNORECASE):\n",
        "        errs.append(\"Must start with SELECT.\")\n",
        "    if not s.endswith(\";\"):\n",
        "        errs.append(\"Must end with ';'.\")\n",
        "    if \";\" in s[:-1]:\n",
        "        errs.append(\"Must contain exactly one statement ending with ';'.\")\n",
        "    if DISALLOWED.search(s):\n",
        "        errs.append(\"Contains disallowed keyword.\")\n",
        "    for ph in re.findall(r\"__\\w+_\\d+__\", s):\n",
        "        if ph not in id_map:\n",
        "            errs.append(f\"Unknown placeholder: {ph}\")\n",
        "    if not any(re.search(rf\"\\b{t}\\b\", s) for t in KNOWN_TABLES):\n",
        "        errs.append(\"No known tables found in SQL.\")\n",
        "    return len(errs) == 0, errs\n",
        "\n",
        "def render_ids(sql: str, id_map: Dict[str,str]) -> str:\n",
        "    out = sql\n",
        "    for ph in sorted(id_map.keys(), key=len, reverse=True):\n",
        "        out = out.replace(ph, id_map[ph])\n",
        "    return out\n",
        "\n",
        "def generate_sql(question: str) -> Dict:\n",
        "    clean_q, id_map = extract_placeholders(question)\n",
        "    id_map_text = id_map_to_text(id_map)\n",
        "    prompt = build_prompt(SCHEMA_TEXT, clean_q, id_map_text)\n",
        "\n",
        "    all_ids, prompt_len = generate_ids(prompt)\n",
        "    completion_ids = all_ids[prompt_len:]  # token-based slicing\n",
        "    completion_text = tokenizer.decode(completion_ids)\n",
        "\n",
        "    sql_ph = extract_sql(completion_text)\n",
        "    ok, errs = validate_sql(sql_ph, id_map)\n",
        "    sql_final = render_ids(sql_ph, id_map) if ok else \"\"\n",
        "\n",
        "    return {\n",
        "        \"original\": question,\n",
        "        \"clean\": clean_q,\n",
        "        \"id_map\": id_map,\n",
        "        \"completion\": completion_text,\n",
        "        \"sql_placeholders\": sql_ph,\n",
        "        \"valid\": ok,\n",
        "        \"errors\": errs,\n",
        "        \"sql_final\": sql_final,\n",
        "    }\n",
        "\n",
        "tests = [\n",
        "    \"How many visits did patient 5432 have in department 25?\",\n",
        "    \"What is the total charge for patient 7890?\",\n",
        "    \"Show monthly visit counts in 2023\",\n",
        "]\n",
        "\n",
        "for i, q in enumerate(tests, 1):\n",
        "    r = generate_sql(q)\n",
        "    print(f\"\\n[Q{i}] {q}\")\n",
        "    print(\"Clean:\", r[\"clean\"])\n",
        "    print(\"ID Map:\", r[\"id_map\"])\n",
        "    print(\"SQL (placeholders):\", r[\"sql_placeholders\"])\n",
        "    print(\"Valid:\", r[\"valid\"])\n",
        "    if not r[\"valid\"]:\n",
        "        print(\"Errors:\", r[\"errors\"])\n",
        "        print(\"Completion preview:\", r[\"completion\"][:220])\n",
        "    else:\n",
        "        print(\"SQL (final):\", r[\"sql_final\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\\\n",
        "# 10) Pass-rate evaluation (synthetic)\n",
        "def sample_questions(n: int = 100) -> List[str]:\n",
        "    out = []\n",
        "    for _ in range(n):\n",
        "        q_tmpl, _ = random.choice(QUESTION_TEMPLATES)\n",
        "        pid = str(rand_id(1000, 9999))\n",
        "        did = str(rand_id(1, 99))\n",
        "        yr  = str(rand_year())\n",
        "        out.append(q_tmpl.format(PID=pid, DID=did, YEAR=yr))\n",
        "    return out\n",
        "\n",
        "def eval_pass_rate(n: int = 100):\n",
        "    qs = sample_questions(n)\n",
        "    passed = 0\n",
        "    reasons = {}\n",
        "    for q in qs:\n",
        "        r = generate_sql(q)\n",
        "        if r[\"valid\"]:\n",
        "            passed += 1\n",
        "        else:\n",
        "            key = r[\"errors\"][0] if r[\"errors\"] else \"Unknown\"\n",
        "            reasons[key] = reasons.get(key, 0) + 1\n",
        "    print(f\"Pass rate: {passed}/{n} = {passed/n:.1%}\")\n",
        "    for k, v in sorted(reasons.items(), key=lambda kv: kv[1], reverse=True)[:10]:\n",
        "        print(f\"- {k}: {v}\")\n",
        "\n",
        "eval_pass_rate(100)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}